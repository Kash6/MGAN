{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kash6/MGAN/blob/main/MDGAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zoK7gvqtJN4w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95d7accd-a1a1-4093-ff91-d2b15dbd826c"
      },
      "source": [
        "!pip install tensorflow==1.13.1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow==1.13.1\n",
            "  Downloading tensorflow-1.13.1-cp37-cp37m-manylinux1_x86_64.whl (92.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 92.6 MB 42 kB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (1.19.5)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (1.1.2)\n",
            "Collecting tensorboard<1.14.0,>=1.13.0\n",
            "  Downloading tensorboard-1.13.1-py3-none-any.whl (3.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2 MB 44.2 MB/s \n",
            "\u001b[?25hCollecting tensorflow-estimator<1.14.0rc0,>=1.13.0\n",
            "  Downloading tensorflow_estimator-1.13.0-py2.py3-none-any.whl (367 kB)\n",
            "\u001b[K     |████████████████████████████████| 367 kB 56.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (3.17.3)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (1.1.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (1.41.1)\n",
            "Requirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (0.12.0)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (0.4.0)\n",
            "Collecting keras-applications>=1.0.6\n",
            "  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n",
            "\u001b[K     |████████████████████████████████| 50 kB 8.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (1.15.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (0.8.1)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (0.37.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.6->tensorflow==1.13.1) (3.1.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (3.3.4)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (1.0.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (4.8.2)\n",
            "Collecting mock>=2.0.0\n",
            "  Downloading mock-4.0.3-py3-none-any.whl (28 kB)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->keras-applications>=1.0.6->tensorflow==1.13.1) (1.5.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (3.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (3.10.0.2)\n",
            "Installing collected packages: mock, tensorflow-estimator, tensorboard, keras-applications, tensorflow\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.7.0\n",
            "    Uninstalling tensorflow-estimator-2.7.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.7.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.7.0\n",
            "    Uninstalling tensorboard-2.7.0:\n",
            "      Successfully uninstalled tensorboard-2.7.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.7.0\n",
            "    Uninstalling tensorflow-2.7.0:\n",
            "      Successfully uninstalled tensorflow-2.7.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "kapre 0.3.5 requires tensorflow>=2.0.0, but you have tensorflow 1.13.1 which is incompatible.\u001b[0m\n",
            "Successfully installed keras-applications-1.0.8 mock-4.0.3 tensorboard-1.13.1 tensorflow-1.13.1 tensorflow-estimator-1.13.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X2LssP7P_RTw",
        "outputId": "07511889-2961-4ebd-b1ec-9b9a49b7889d"
      },
      "source": [
        "!pip uninstall kapre"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: kapre 0.3.5\n",
            "Uninstalling kapre-0.3.5:\n",
            "  Would remove:\n",
            "    /usr/local/lib/python3.7/dist-packages/kapre-0.3.5.dist-info/*\n",
            "    /usr/local/lib/python3.7/dist-packages/kapre/*\n",
            "Proceed (y/n)? y\n",
            "  Successfully uninstalled kapre-0.3.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sr2EG4D-x3Ye"
      },
      "source": [
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "from __future__ import absolute_import\n",
        "\n",
        "import numpy as np\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "plt.switch_backend('agg')\n",
        "plt.style.use('ggplot')\n",
        "\n",
        "class Prior(object):\n",
        "    def __init__(self, type):\n",
        "        self.type = type\n",
        "\n",
        "    def sample(self, shape):\n",
        "        if self.type == \"uniform\":\n",
        "            return np.random.uniform(-1.0, 1.0, shape)\n",
        "        else:\n",
        "            return np.random.normal(0, 1, shape)\n",
        "\n",
        "def conv_out_size_same(size, stride):\n",
        "    return int(math.ceil(float(size) / float(stride)))\n",
        "\n",
        "def make_batches(size, batch_size):\n",
        "    '''Returns a list of batch indices (tuples of indices).\n",
        "    '''\n",
        "    return [(i, min(size, i + batch_size)) for i in range(0, size, batch_size)]\n",
        "\n",
        "def create_image_grid(x, img_size, tile_shape):\n",
        "    assert (x.shape[0] == tile_shape[0] * tile_shape[1])\n",
        "    assert (x[0].shape == img_size)\n",
        "\n",
        "    img = np.zeros((img_size[0] * tile_shape[0] + tile_shape[0] - 1,\n",
        "                    img_size[1] * tile_shape[1] + tile_shape[1] - 1,\n",
        "                    3))\n",
        "\n",
        "    for t in range(x.shape[0]):\n",
        "        i, j = t // tile_shape[1], t % tile_shape[1]\n",
        "        img[i * img_size[0] + i : (i + 1) * img_size[0] + i, j * img_size[1] + j : (j + 1) * img_size[1] + j] = x[t]\n",
        "\n",
        "    return img\n",
        "\n",
        "\n",
        "def disp_scatter(x, g, gen, num_gens, fig=None, ax=None):\n",
        "    colors = ['darkblue', 'yellow', 'indigo', 'darkgreen', 'purple',\n",
        "              'dodgerblue', 'lime', 'brown', 'darkcyan', 'deeppink']\n",
        "\n",
        "    if ax is None:\n",
        "        fig, ax = plt.subplots()\n",
        "\n",
        "    ax.cla()\n",
        "    ax.scatter(x[:, 0], x[:, 1], s=10, marker='+', color='r', alpha=0.8)\n",
        "    for i in range(num_gens):\n",
        "        ax.scatter(g[gen == i, 0], g[gen == i, 1], s=10, marker='o',\n",
        "                   color=colors[i], alpha=0.8)\n",
        "    ax.legend([\"real data\"] + ['gen {}'.format(i) for i in range(num_gens)])\n",
        "    ax.set_xlim(-3, 3)\n",
        "    ax.set_ylim(-3, 3)\n",
        "    return fig, ax"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RyCuICc4GHY2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ff4c9ff-d518-4372-c02c-bb7e8d800d49"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "def lrelu(x, alpha=0.2):\n",
        "    return tf.maximum(x, alpha * x)\n",
        "\n",
        "def linear(input, output_dim, scope='linear', stddev=0.01):\n",
        "    norm = tf.random_normal_initializer(stddev=stddev)\n",
        "    const = tf.constant_initializer(0.0)\n",
        "    with tf.variable_scope(scope):\n",
        "        w = tf.get_variable('weights', [input.get_shape()[1], output_dim], initializer=norm)\n",
        "        b = tf.get_variable('biases', [output_dim], initializer=const)\n",
        "        return tf.matmul(input, w) + b\n",
        "\n",
        "def conv2d(input_, output_dim,\n",
        "           k_h=5, k_w=5, d_h=2, d_w=2, stddev=0.02,\n",
        "           name=\"conv2d\"):\n",
        "    with tf.variable_scope(name):\n",
        "        w = tf.get_variable('weights', [k_h, k_w, input_.get_shape()[-1], output_dim],\n",
        "                            initializer=tf.truncated_normal_initializer(stddev=stddev))\n",
        "        conv = tf.nn.conv2d(input_, w, strides=[1, d_h, d_w, 1], padding='SAME')\n",
        "\n",
        "        biases = tf.get_variable('biases', [output_dim], initializer=tf.constant_initializer(0.0))\n",
        "        # conv = tf.reshape(tf.nn.bias_add(conv, biases), conv.get_shape())\n",
        "\n",
        "        return tf.nn.bias_add(conv, biases)\n",
        "\n",
        "\n",
        "def deconv2d(input_, output_shape,\n",
        "             k_h=5, k_w=5, d_h=2, d_w=2, stddev=0.02,\n",
        "             name=\"deconv2d\", with_w=False):\n",
        "    with tf.variable_scope(name):\n",
        "        # filter : [height, width, output_channels, in_channels]\n",
        "        w = tf.get_variable('weights', [k_h, k_w, output_shape[-1], input_.get_shape()[-1]],\n",
        "                            initializer=tf.random_normal_initializer(stddev=stddev))\n",
        "\n",
        "        try:\n",
        "            deconv = tf.nn.conv2d_transpose(input_, w, output_shape=output_shape,\n",
        "                                            strides=[1, d_h, d_w, 1])\n",
        "\n",
        "        # Support for versions of TensorFlow before 0.7.0\n",
        "        except AttributeError:\n",
        "            deconv = tf.nn.deconv2d(input_, w, output_shape=output_shape,\n",
        "                                    strides=[1, d_h, d_w, 1])\n",
        "\n",
        "        biases = tf.get_variable('biases', [output_shape[-1]],\n",
        "                                 initializer=tf.constant_initializer(0.0))\n",
        "        deconv = tf.reshape(tf.nn.bias_add(deconv, biases), deconv.get_shape())\n",
        "\n",
        "        if with_w:\n",
        "            return deconv, w, biases\n",
        "        else:\n",
        "            return deconv\n",
        "\n",
        "def gmm_sample(num_samples, mix_coeffs, mean, cov):\n",
        "    z = np.random.multinomial(num_samples, mix_coeffs)\n",
        "    samples = np.zeros(shape=[num_samples, len(mean[0])])\n",
        "    i_start = 0\n",
        "    for i in range(len(mix_coeffs)):\n",
        "        i_end = i_start + z[i]\n",
        "        samples[i_start:i_end, :] = np.random.multivariate_normal(\n",
        "            mean=np.array(mean)[i, :],\n",
        "            cov=np.diag(np.array(cov)[i, :]),\n",
        "            size=z[i])\n",
        "        i_start = i_end\n",
        "    return samples"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "csXjIBs7GclW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40fbb970-7480-4f0c-a8dc-7181354d1970"
      },
      "source": [
        "from functools import partial\n",
        "\n",
        "import os\n",
        "\n",
        "\n",
        "batch_norm = partial(tf.contrib.layers.batch_norm,\n",
        "                     decay=0.9,\n",
        "                     updates_collections=None,\n",
        "                     epsilon=1e-5,\n",
        "                     scale=True)\n",
        "\n",
        "\n",
        "class MGAN(object):\n",
        "    \"\"\"Mixture Generative Adversarial Nets\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 model_name='MGAN',\n",
        "                 beta=1.0,\n",
        "                 num_z=128,\n",
        "                 num_gens=4,\n",
        "                 d_batch_size=64,\n",
        "                 g_batch_size=32,\n",
        "                 z_prior=\"uniform\",\n",
        "                 same_input=True,\n",
        "                 learning_rate=0.0002,\n",
        "                 img_size=(64, 64, 3),  # (height, width, channels)\n",
        "                 num_conv_layers=3,\n",
        "                 num_gen_feature_maps=128,  # number of feature maps of generator\n",
        "                 num_dis_feature_maps=128,  # number of feature maps of discriminator\n",
        "                 sample_fp=None,\n",
        "                 sample_by_gen_fp=None,\n",
        "                 num_epochs=25000,\n",
        "                 random_seed=6789):\n",
        "        self.beta = beta\n",
        "        self.num_z = num_z\n",
        "        self.num_gens = num_gens\n",
        "        self.d_batch_size = d_batch_size\n",
        "        self.g_batch_size = g_batch_size\n",
        "        self.z_prior = Prior(z_prior)\n",
        "        self.same_input = same_input\n",
        "        self.learning_rate = learning_rate\n",
        "        self.num_epochs = num_epochs\n",
        "        self.img_size = img_size\n",
        "        self.num_conv_layers = num_conv_layers\n",
        "        self.num_gen_feature_maps = num_gen_feature_maps\n",
        "        self.num_dis_feature_maps = num_dis_feature_maps\n",
        "        self.sample_fp = sample_fp\n",
        "        self.sample_by_gen_fp = sample_by_gen_fp\n",
        "        self.random_seed = random_seed\n",
        "\n",
        "    def _init(self):\n",
        "        self.epoch = 0\n",
        "\n",
        "        # TensorFlow's initialization\n",
        "        self.tf_graph = tf.Graph()\n",
        "        self.tf_config = tf.ConfigProto()\n",
        "        self.tf_config.gpu_options.allow_growth = True\n",
        "        self.tf_config.log_device_placement = False\n",
        "        self.tf_config.allow_soft_placement = True\n",
        "        self.tf_session = tf.Session(config=self.tf_config, graph=self.tf_graph)\n",
        "\n",
        "        np.random.seed(self.random_seed)\n",
        "        with self.tf_graph.as_default():\n",
        "            tf.set_random_seed(self.random_seed)\n",
        "\n",
        "    def _build_model(self):\n",
        "        arr = np.array([i // self.g_batch_size for i in range(self.g_batch_size * self.num_gens)])\n",
        "        d_mul_labels = tf.constant(arr, dtype=tf.int32)\n",
        "\n",
        "        self.x = tf.placeholder(tf.float32, [None,\n",
        "                                             self.img_size[0], self.img_size[1], self.img_size[2]],\n",
        "                                name=\"real_data\")\n",
        "        self.z = tf.placeholder(tf.float32, [self.g_batch_size * self.num_gens, self.num_z], name='noise')\n",
        "\n",
        "        # create generator G\n",
        "        self.g = self._create_generator(self.z)\n",
        "\n",
        "        # create sampler to generate samples\n",
        "        self.sampler = self._create_generator(self.z, train=False, reuse=True)\n",
        "\n",
        "        # create discriminator D\n",
        "        d_bin_x_logits, d_mul_x_logits = self._create_discriminator(self.x)\n",
        "        d_bin_g_logits, d_mul_g_logits = self._create_discriminator(self.g, reuse=True)\n",
        "\n",
        "        # define loss functions\n",
        "        self.d_bin_x_loss = tf.reduce_mean(\n",
        "            tf.nn.sigmoid_cross_entropy_with_logits(\n",
        "                logits=d_bin_x_logits, labels=tf.ones_like(d_bin_x_logits)),\n",
        "            name='d_bin_x_loss')\n",
        "        self.d_bin_g_loss = tf.reduce_mean(\n",
        "            tf.nn.sigmoid_cross_entropy_with_logits(\n",
        "                logits=d_bin_g_logits, labels=tf.zeros_like(d_bin_g_logits)),\n",
        "            name='d_bin_g_loss')\n",
        "        self.d_bin_loss = tf.add(self.d_bin_x_loss, self.d_bin_g_loss, name='d_bin_loss')\n",
        "        self.d_mul_loss = tf.reduce_mean(\n",
        "            tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
        "                logits=d_mul_g_logits, labels=d_mul_labels),\n",
        "            name=\"d_mul_loss\")\n",
        "        self.d_loss = tf.add(self.d_bin_loss, self.d_mul_loss, name=\"d_loss\")\n",
        "\n",
        "        self.g_bin_loss = tf.reduce_mean(\n",
        "            tf.nn.sigmoid_cross_entropy_with_logits(\n",
        "                logits=d_bin_g_logits, labels=tf.ones_like(d_bin_g_logits)),\n",
        "            name=\"g_bin_loss\")\n",
        "        self.g_mul_loss = tf.multiply(self.beta, self.d_mul_loss, name='g_mul_loss')\n",
        "        self.g_loss = tf.add(self.g_bin_loss, self.g_mul_loss, name=\"g_loss\")\n",
        "\n",
        "        # create optimizers\n",
        "        self.d_opt = self._create_optimizer(self.d_loss, scope='discriminator',\n",
        "                                            lr=self.learning_rate)\n",
        "        self.g_opt = self._create_optimizer(self.g_loss, scope='generator',\n",
        "                                            lr=self.learning_rate)\n",
        "\n",
        "    def _create_generator(self, z, train=True, reuse=False, name=\"generator\"):\n",
        "        out_size = [(conv_out_size_same(self.img_size[0], 2),\n",
        "                     conv_out_size_same(self.img_size[1], 2),\n",
        "                     self.num_gen_feature_maps)]\n",
        "        for i in range(self.num_conv_layers - 1):\n",
        "            out_size = [(conv_out_size_same(out_size[0][0], 2),\n",
        "                         conv_out_size_same(out_size[0][1], 2),\n",
        "                         out_size[0][2] * 2)] + out_size\n",
        "\n",
        "        with tf.variable_scope(name) as scope:\n",
        "            if reuse:\n",
        "                scope.reuse_variables()\n",
        "\n",
        "            z_split = tf.split(z, self.num_gens, axis=0)\n",
        "            h0 = []\n",
        "            for i, var in enumerate(z_split):\n",
        "                h0.append(tf.nn.relu(batch_norm(linear(var, out_size[0][0] * out_size[0][1] * out_size[0][2],\n",
        "                                                       scope='g_h0_linear{}'.format(i), stddev=0.02),\n",
        "                                                is_training=train,\n",
        "                                                scope=\"g_h0_bn{}\".format(i)),\n",
        "                                     name=\"g_h0_relu{}\".format(i)))\n",
        "\n",
        "            h = []\n",
        "            for var in h0:\n",
        "                h.append(tf.reshape(var, [self.g_batch_size, out_size[0][0], out_size[0][1], out_size[0][2]]))\n",
        "            h = tf.concat(h, axis=0, name=\"g_h0_relu\")\n",
        "\n",
        "            for i in range(1, self.num_conv_layers):\n",
        "                h = tf.nn.relu(\n",
        "                    batch_norm(\n",
        "                        deconv2d(h,\n",
        "                                 [self.g_batch_size  * self.num_gens, out_size[i][0], out_size[i][1], out_size[i][2]],\n",
        "                                 stddev=0.02, name=\"g_h{}_deconv\".format(i)),\n",
        "                        is_training=train,\n",
        "                        center=False,\n",
        "                        scope=\"g_h{}_bn\".format(i)),\n",
        "                    name=\"g_h{}_relu\".format(i))\n",
        "\n",
        "            g_out = tf.nn.tanh(\n",
        "                deconv2d(h,\n",
        "                         [self.g_batch_size * self.num_gens, self.img_size[0], self.img_size[1], self.img_size[2]],\n",
        "                         stddev=0.02, name=\"g_out_deconv\"),\n",
        "                name=\"g_out_tanh\")\n",
        "            return g_out\n",
        "\n",
        "    def _create_discriminator(self, x, train=True, reuse=False, name=\"discriminator\"):\n",
        "        with tf.variable_scope(name) as scope:\n",
        "            if reuse:\n",
        "                scope.reuse_variables()\n",
        "\n",
        "            h = x\n",
        "            for i in range(self.num_conv_layers):\n",
        "                h = lrelu(batch_norm(conv2d(h, self.num_dis_feature_maps * (2 ** i),\n",
        "                                            stddev=0.02, name=\"d_h{}_conv\".format(i)),\n",
        "                                     is_training=train,\n",
        "                                     scope=\"d_bn{}\".format(i)))\n",
        "\n",
        "            dim = h.get_shape()[1:].num_elements()\n",
        "            h = tf.reshape(h, [-1, dim])\n",
        "            d_bin_logits = linear(h, 1, scope='d_bin_logits')\n",
        "            d_mul_logits = linear(h, self.num_gens, scope='d_mul_logits')\n",
        "        return d_bin_logits, d_mul_logits\n",
        "\n",
        "    def _create_optimizer(self, loss, scope, lr):\n",
        "        params = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=scope)\n",
        "        opt = tf.train.AdamOptimizer(lr, beta1=0.5)\n",
        "        grads = opt.compute_gradients(loss, var_list=params)\n",
        "        train_op = opt.apply_gradients(grads)\n",
        "        return train_op\n",
        "\n",
        "    def fit(self, x):\n",
        "\n",
        "            if (not hasattr(self, 'epoch')) or self.epoch == 0:\n",
        "                self._init()\n",
        "                with self.tf_graph.as_default() as g:\n",
        "                    self._build_model()\n",
        "                    saver = tf.compat.v1.train.Saver()\n",
        "                    self.tf_session.run(tf.global_variables_initializer())\n",
        "            num_data = x.shape[0] - x.shape[0] % self.d_batch_size\n",
        "            batches = make_batches(num_data, self.d_batch_size)\n",
        "            best_is = 0.0\n",
        "            while (self.epoch < self.num_epochs):\n",
        "                for batch_idx, (batch_start, batch_end) in enumerate(batches):\n",
        "                    batch_size = batch_end - batch_start\n",
        "\n",
        "                    x_batch = x[batch_start:batch_end]\n",
        "                    if self.same_input:\n",
        "                        z_batch = self.z_prior.sample([self.g_batch_size, self.num_z]).astype(np.float32)\n",
        "                        z_batch = np.vstack([z_batch] * self.num_gens)\n",
        "                    else:\n",
        "                        z_batch = self.z_prior.sample([self.g_batch_size * self.num_gens, self.num_z]).astype(np.float32)\n",
        "\n",
        "                    # update discriminator D\n",
        "                    d_bin_loss, d_mul_loss, d_loss, _ = self.tf_session.run(\n",
        "                        [self.d_bin_loss, self.d_mul_loss, self.d_loss, self.d_opt],\n",
        "                        feed_dict={self.x: x_batch, self.z: z_batch})\n",
        "\n",
        "                    # update generator G\n",
        "                    g_bin_loss, g_mul_loss, g_loss, _ = self.tf_session.run(\n",
        "                        [self.g_bin_loss, self.g_mul_loss, self.g_loss, self.g_opt],\n",
        "                        feed_dict={self.z: z_batch})\n",
        "                    saver.save(self.tf_session,'my-model.ckpt',global_step=10)\n",
        "                self.epoch += 1\n",
        "                print(\"Epoch: [%4d/%4d] d_bin_loss: %.5f, d_mul_loss: %.5f, d_loss: %.5f,\"\n",
        "                      \" g_bin_loss: %.5f, g_mul_loss: %.5f, g_loss: %.5f\" % (self.epoch, self.num_epochs,\n",
        "                                    d_bin_loss, d_mul_loss, d_loss, g_bin_loss, g_mul_loss, g_loss))\n",
        "                self._samples(self.sample_fp.format(epoch=self.epoch+1))\n",
        "                self._samples_by_gen(self.sample_by_gen_fp.format(epoch=self.epoch+1))\n",
        "\n",
        "    def _generate(self, num_samples=100):\n",
        "        sess = self.tf_session\n",
        "        batch_size = self.g_batch_size * self.num_gens\n",
        "        num = ((num_samples - 1) // batch_size + 1) * batch_size\n",
        "        z = self.z_prior.sample([num, self.num_z]).astype(np.float32)\n",
        "        x = np.zeros([num, self.img_size[0], self.img_size[1], self.img_size[2]],\n",
        "                     dtype=np.float32)\n",
        "        batches = make_batches(num, batch_size)\n",
        "        for batch_idx, (batch_start, batch_end) in enumerate(batches):\n",
        "            z_batch = z[batch_start:batch_end]\n",
        "            x[batch_start:batch_end] = sess.run(self.sampler,\n",
        "                                                feed_dict={self.z: z_batch})\n",
        "        idx = np.random.permutation(num)[:num_samples]\n",
        "        x = (x[idx] + 1.0) / 2.0\n",
        "        return x\n",
        "\n",
        "    def _samples(self, filepath, tile_shape=(1, 1)):\n",
        "        if not os.path.exists(os.path.dirname(filepath)):\n",
        "            os.makedirs(os.path.dirname(filepath))\n",
        "\n",
        "        num_samples = tile_shape[1] * tile_shape[1]\n",
        "        x = self._generate(num_samples)\n",
        "        imgs = create_image_grid(x, img_size=self.img_size, tile_shape=tile_shape)\n",
        "        import imageio\n",
        "        imageio.imsave(filepath, imgs)\n",
        "\n",
        "    def _samples_by_gen(self, filepath):\n",
        "        if not os.path.exists(os.path.dirname(filepath)):\n",
        "            os.makedirs(os.path.dirname(filepath))\n",
        "\n",
        "        num_samples = self.num_gens * 1\n",
        "        tile_shape = (self.num_gens, 1)\n",
        "\n",
        "        sess = self.tf_session\n",
        "        img_per_gen = num_samples // self.num_gens\n",
        "        x = np.zeros([num_samples, self.img_size[0], self.img_size[1], self.img_size[2]],\n",
        "                     dtype=np.float32)\n",
        "        for i in range(0, img_per_gen, self.g_batch_size):\n",
        "            z_batch = self.z_prior.sample([self.g_batch_size * self.num_gens, self.num_z]).astype(np.float32)\n",
        "            samples = sess.run(self.sampler, feed_dict={self.z: z_batch})\n",
        "\n",
        "            for gen in range(self.num_gens):\n",
        "                x[gen * img_per_gen + i:gen * img_per_gen + min(i + self.g_batch_size, img_per_gen)] = \\\n",
        "                    samples[\n",
        "                    gen * self.g_batch_size:gen * self.g_batch_size + min(self.g_batch_size, img_per_gen)]\n",
        "\n",
        "        x = (x + 1.0) / 2.0\n",
        "        imgs = create_image_grid(x, img_size=self.img_size, tile_shape=tile_shape)\n",
        "        import imageio\n",
        "        imageio.imsave(filepath, imgs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YGG0UraIGoR6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 229
        },
        "outputId": "c226b212-92f6-4efe-e68d-a3365efdd413"
      },
      "source": [
        "import sys\n",
        "import pickle\n",
        "import argparse\n",
        "\n",
        "tmp = pickle.load(open(\"/content/L2.pkl\", \"rb\"))\n",
        "x_train = tmp.astype(np.float32).reshape([-1, 64, 64, 3]) / 127.5 - 1.\n",
        "model = MGAN(\n",
        "        num_z=128,\n",
        "        beta=1.0,\n",
        "        num_gens=4,\n",
        "        d_batch_size=64,\n",
        "        g_batch_size=32,\n",
        "        z_prior=\"uniform\",\n",
        "        learning_rate=0.0002,\n",
        "        img_size=(64, 64, 3),\n",
        "        num_conv_layers=3,\n",
        "        num_gen_feature_maps=128,\n",
        "        num_dis_feature_maps=128,\n",
        "        num_epochs=25000,\n",
        "        sample_fp=\"samples/samples_{epoch:04d}.png\",\n",
        "        sample_by_gen_fp=\"samples_by_gen/samples_{epoch:04d}.png\",\n",
        "        random_seed=6789)\n",
        "model.fit(x_train)\n",
        " "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-97679e41bf73>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0margparse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtmp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/L2.pkl\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mx_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtmp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m127.5\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m model = MGAN(\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/L2.pkl'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GsTaAi1wGoY1"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sozKVdJnprHM"
      },
      "source": [
        " "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}